{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/johndinovi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/johndinovi/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/johndinovi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/johndinovi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "import tensorflow as tf\n",
    "nltk.download('stopwords')\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Some Functionality for Data Cleaning\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to Remove Punctuation\n",
    "def remove_punctuation(text):\n",
    "  '''\n",
    "  Input:\n",
    "    'text': a string of characters\n",
    "  Output:\n",
    "    'text', but removed of punctuation\n",
    "  '''\n",
    "  text = str(text)\n",
    "  for punc in string.punctuation:\n",
    "      if punc in text:\n",
    "          text = text.replace(punc, ' ')\n",
    "  return text.strip()\n",
    "\n",
    "# Function to Tokenize Text and Remove Stop Words from Text\n",
    "def remove_stop_words_and_tokenize(text):\n",
    "  '''\n",
    "  Input: \n",
    "    'text': a string removed of punctuation\n",
    "  Output: \n",
    "    Tokenized version of 'text' and with stop words removed\n",
    "  '''\n",
    "  text = text.lower()\n",
    "  tokenized = word_tokenize(text)\n",
    "  no_stops = []\n",
    "  for word in tokenized:\n",
    "    if word not in stop_words:\n",
    "      no_stops.append(word)\n",
    "  return no_stops\n",
    "\n",
    "# Function to Stem Tokenized text\n",
    "def stem(tokenized_text):\n",
    "  '''\n",
    "  Input: \n",
    "    'tokenized_text': a list of a tokenized feature\n",
    "  Output: \n",
    "    Stemmed version of 'tokenized_text'\n",
    "  '''\n",
    "  for i, word in enumerate(tokenized_text):\n",
    "    tokenized_text[i] = stemmer.stem(word)\n",
    "  return tokenized_text\n",
    "\n",
    "# Function to Lemmatize Tokenized text\n",
    "def lemmatize(tokenized_text):\n",
    "  '''\n",
    "  Input: \n",
    "    'tokenized_text': a list of a tokenized feature\n",
    "  Output: \n",
    "    Lemmatized version of 'tokenized_text'\n",
    "  '''\n",
    "  for i, word in enumerate(tokenized_text):\n",
    "    tokenized_text[i] = lemmatizer.lemmatize(word)\n",
    "  return tokenized_text\n",
    "\n",
    "# ---------------------------------------------------------- #\n",
    "\n",
    "# Function to Return Cleaned Text\n",
    "def clean(text):\n",
    "  '''\n",
    "  Input: \n",
    "    'text': an input string\n",
    "  Output: \n",
    "    'text' with removed punctuation, tokenized,\n",
    "          stop words removed, and lemmatized\n",
    "  '''\n",
    "  text = remove_punctuation(text)\n",
    "  text = remove_stop_words_and_tokenize(text)\n",
    "  text = lemmatize(text)\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Bag of Words Model\n",
    "def build_set(feature_list):\n",
    "  '''\n",
    "  Input: \n",
    "    'feature_list': list of tuples of uncleaned input (0) text and sentiment (1)\n",
    "  Output: \n",
    "    Set of all words in the combined and cleaned strings from 'feature_list'\n",
    "  '''\n",
    "  total_string = \"\"\n",
    "  for feature in feature_list:\n",
    "    if type(feature[0]) == type(\" \"):\n",
    "      total_string += \" \" + feature[0]\n",
    "  total_string = clean(total_string)\n",
    "  return set(total_string)\n",
    "\n",
    "# Function to Build a BoW Vector for a Given Feature and Set of Words\n",
    "def build_vector(feature, word_set):\n",
    "  '''\n",
    "  Input:\n",
    "    'feature'(list): a cleaned string of input\n",
    "    'word_set'(set): a set of all clean words seen over all features\n",
    "  Output:\n",
    "    A bag of words vectorized version of a given feature\n",
    "  '''\n",
    "  vector = np.zeros((len(word_set),))\n",
    "  if feature:\n",
    "    word_counter = collections.Counter(feature)\n",
    "    for i, word in enumerate(word_set):\n",
    "      vector[i] += word_counter[word]\n",
    "  return vector\n",
    "\n",
    "def bow_matrix(feature_list, myset=None):\n",
    "  '''\n",
    "  Input:\n",
    "    'feature_list': a list of features to train the model on\n",
    "  Output:\n",
    "    A matrix of the BoW representation of each feature\n",
    "  '''\n",
    "  if not myset:\n",
    "    myset = build_set(feature_list)\n",
    "  bow_mat = []\n",
    "  y_vec = []\n",
    "  for i, feature in enumerate(feature_list):\n",
    "    y_vec.append(feature[1])\n",
    "    feature = clean(feature[0])\n",
    "    vector = build_vector(feature, myset)\n",
    "    bow_mat.append(np.array(vector))\n",
    "  return np.array(bow_mat), np.array(y_vec), myset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Handle the Data\n",
    "def process_data(data):\n",
    "  '''\n",
    "  Input:\n",
    "    'data': a pandas data frame with entries 'text' and 'sentiment'\n",
    "  Output:\n",
    "    A processed form of all the features with just the 'text' and 'sentiment'\n",
    "    values zipped together\n",
    "  '''\n",
    "  out_text = []\n",
    "  out_sents = []\n",
    "  text = np.array(data['text'])\n",
    "  sentiment = np.array(data['sentiment'])\n",
    "  for i, sent in enumerate(sentiment):\n",
    "    if sent == \"positive\":\n",
    "      out_sents.append([0, 0, 1])\n",
    "    elif sent == \"neutral\":\n",
    "      out_sents.append([0, 1, 0])\n",
    "    elif sent == \"negative\":\n",
    "      out_sents.append([1, 0, 0])\n",
    "    out_text.append(text[i])\n",
    "  feature_list = zip(out_text, out_sents)\n",
    "  return tuple(feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Twitter Data\n",
    "train_data = pd.read_csv('./train.csv', encoding='unicode_escape')\n",
    "test_data = pd.read_csv('./test.csv', encoding='unicode_escape')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the Training and Testing Data\n",
    "train_features = process_data(train_data)\n",
    "test_features = process_data(test_data)\n",
    "\n",
    "x_train, y_train, mydict = bow_matrix(train_features)\n",
    "with open('../mydict.txt', 'w') as f:\n",
    "    for word in mydict:\n",
    "        f.write(str(word) + ',')\n",
    "x_test, y_test, _ = bow_matrix(test_features, mydict)\n",
    "\n",
    "# Normalize the inputs\n",
    "x_train = tf.keras.utils.normalize(x_train, axis=1)\n",
    "x_test = tf.keras.utils.normalize(x_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23903\n"
     ]
    }
   ],
   "source": [
    "feature_size = len(x_train[0]) #23903\n",
    "print(feature_size)\n",
    "input_dim = (feature_size,)\n",
    "\n",
    "def build_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(256, activation=tf.nn.relu, input_shape=input_dim),\n",
    "        tf.keras.layers.Dense(256, activation=tf.nn.relu),\n",
    "        tf.keras.layers.Dense(256, activation=tf.nn.relu),\n",
    "        tf.keras.layers.Dense(3, activation=tf.nn.softmax)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "model = build_model()\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=0.0001),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "859/859 [==============================] - 14s 16ms/step - loss: 0.0713 - accuracy: 0.9813\n",
      "Epoch 2/3\n",
      "859/859 [==============================] - 13s 16ms/step - loss: 0.0415 - accuracy: 0.9888\n",
      "Epoch 3/3\n",
      "859/859 [==============================] - 13s 16ms/step - loss: 0.0272 - accuracy: 0.9929\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x349454760>"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111/111 [==============================] - 1s 9ms/step - loss: 1.6462 - accuracy: 0.6497\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.646216630935669, 0.649688720703125]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('./saved_weights/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "import random\n",
    "\n",
    "# Function to Simulate Slow Typing\n",
    "typing_speed = 500 #wpm\n",
    "def slow_print(t):\n",
    "    for l in t:\n",
    "        sys.stdout.write(l)\n",
    "        sys.stdout.flush()\n",
    "        time.sleep(random.random()*10.0/typing_speed)\n",
    "    print('')\n",
    "\n",
    "def blendiebot(model1):\n",
    "    slow_print(\"Hello! My name is BlendieBot!\\n\") \n",
    "    time.sleep(1)\n",
    "    slow_print(\"I am an AI model here to see how your are feeling and to provide helpful strategies for navigating this crazy world!\\n\")\n",
    "    time.sleep(1)\n",
    "    slow_print(\"If you would like to quit ant any time, just type 'quit' whenever asked how you are doing.\\n\")\n",
    "    while True:\n",
    "      slow_print(\"Give as much information as possible and be explicit. Avoid double negatives and 'not' phrases!\\n\")\n",
    "      slow_print(\"How are you doing?\\n\")\n",
    "      response = input()\n",
    "      if response.lower() == 'quit':\n",
    "        break\n",
    "\n",
    "      response = clean(response)\n",
    "      response = np.array([build_vector(response, mydict)])\n",
    "      score = np.argmax(model1.predict(response))\n",
    "\n",
    "      if score == 2:\n",
    "        slow_print(\"I am glad to hear that you are doing well today. Would you like to dive deeper into your thoughts? (y or n)\\n\")\n",
    "        response = ''\n",
    "        while response.lower() != \"y\" and response.lower != \"n\":\n",
    "          response = input()\n",
    "          if response.lower() == \"y\":\n",
    "             break\n",
    "          if response.lower() == \"n\":\n",
    "            slow_print(\"If you are ever feeling down, please come and talk to me! I am always happy to help!\\n\")\n",
    "            time.sleep(1)\n",
    "            slow_print(\"Goodbye!\")\n",
    "            return\n",
    "      elif score == 0:\n",
    "        slow_print(\"I am sorry to hear that you are not feeling your best.\\n\")\n",
    "        time.sleep(1)\n",
    "        response = ''\n",
    "        while response.lower() != \"y\" and response.lower != \"n\":\n",
    "          slow_print(\"Would you like some suggestions to work through what you are feeling? (y or n) \\n\")\n",
    "          response = input()\n",
    "          if response.lower() == \"y\":\n",
    "             response = ''\n",
    "             while response.lower() != \"cbt\" and response.lower != \"ppt\":\n",
    "              slow_print(\"Would you like Psychodynamic Psychotherapy techniques, or Cognitive Behavioral Therapy techniques? (PPT or CBT)\\n\")\n",
    "              response = input()\n",
    "              if response.lower() == \"ppt\":\n",
    "                slow_print(\"If you are ever in serious trouble, please call 911. Feeling down is not an easy task to navigate.\\n\")\n",
    "                slow_print(\"I suggest you do your best to explore why you are feeling the way that you are, and ask yourself thought provoking questions.\\n\")\n",
    "                slow_print(\"Perhaps reach out to a family member or a close friend, with their consent, to discuss how the relationships in your life may be impacting your feelings.\\n\")\n",
    "                slow_print(\"Lastly, if you are feeling up to it, consider seeing an actual therapist to further help with these challenging thoughts. Remember, there are always people who care about you.\\n\")\n",
    "                break\n",
    "              elif response.lower() == \"cbt\":\n",
    "                slow_print(\"If you are ever in serious trouble, please call 911. Feeling down is not an easy task to navigate.\\n\")\n",
    "                slow_print(\"You are experiencing some difficult negative thoughts and emotions, but it is important to remember that they are distorted and not worth entertaining.\\n\") \n",
    "                slow_print(\"At this link, <https://tinyurl.com/64c8eycs>, you will find a sheet to help work through these thoughts and make them more closely align with reality.\\n\")\n",
    "                slow_print(\"Lastly, if you are feeling up to it, consider seeing an actual therapist to further help with these challenging times. Remember, there are always people who care about you.\\n\")\n",
    "                break\n",
    "          if response.lower() == \"n\":\n",
    "             slow_print(\"No worries. I am always here to talk!\\n\")\n",
    "             break\n",
    "        break\n",
    "      else:\n",
    "        slow_print(\"It seems that you are feeling pretty neutral. Describe in more detail how you are doing. \\n\")\n",
    "    slow_print(\"I hope I was able to help!\\n\\nFeel free to come back and talk about whatever is bothering you!\\n\")\n",
    "    slow_print(\"Goodbye!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x349454640>"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1 = build_model()\n",
    "model1.load_weights('./saved_weights/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! My name is BlendieBot!\n",
      "\n",
      "I am an AI model here to see how your are feeling and to provide helpful strategies for navigating this crazy world!\n",
      "\n",
      "If you would like to quit ant any time, just type 'quit' whenever asked how you are doing.\n",
      "\n",
      "Give as much information as possible and be explicit. Avoid double negatives and 'not' phrases!\n",
      "\n",
      "How are you doing?\n",
      "\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "I am glad to hear that you are doing well today. Would you like to dive deeper into your thoughts? (y or n)\n",
      "\n",
      "If you are ever feeling down, please come and talk to me! I am always happy to help!\n",
      "\n",
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "blendiebot(model1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
